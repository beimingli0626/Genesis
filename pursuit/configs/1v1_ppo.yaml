train_cfg:
  algorithm:
    class_name: PPO
    clip_param: 0.2
    desired_kl: 0.01
    entropy_coef: 0.0
    gamma: 0.99
    lam: 0.95
    learning_rate: 0.0001
    max_grad_norm: 1.0
    num_learning_epochs: 5
    num_mini_batches: 4
    schedule: adaptive
    use_clipped_value_loss: true
    value_loss_coef: 1.0

  policy:
    class_name: ActorCritic
    activation: elu
    actor_hidden_dims: [512, 256, 128]
    critic_hidden_dims: [512, 256, 128]
    init_noise_std: 1.0

  num_steps_per_env: 10
  num_learning_iterations: 1000
  empirical_normalization: false
  # -- logging parameters
  save_interval: 100
  experiment_name: "capture_the_flag"
  run_name: ""
  # -- logging  writer
  logger: "wandb"
  wandb_project: "capture-the-flag"
  # -- load and resuming
  resume: false
  load_run: -1
  resume_path: null
  checkpoint: -1

env_cfg:
  num_envs: 1024                # number of environments to run in parallel
  episode_length_s: 10.0        # episode length in seconds
  dt: 0.01                      # simulation timestep
  step_dt: 0.1                  # control timestep
  agent:
    num_agents: 1               # number of agents
    num_observations: 6         # number of observations per agent
    num_actions: 3              # number of actions per agent
    tag_threshold: 0.5          # distance threshold for tagging
    clip_agent_actions: 2.0     # action clipping range, velocity
  arena:
    arena_size: 4.0            # edge length of arena, which would be a square
  reward:
    reward_scales:             # reward scaling factors
      tag: 1                 # award the pursuer for capturing the target, -1 if not captured
  max_visualize_FPS: 60       # maximum FPS for visualization
