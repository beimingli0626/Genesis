train_cfg:
  num_episodes: 300            # number of episodes to train for
  rollouts: 10                 # number of rollouts before updating
  learning_epochs: 5           # number of learning epochs during each update
  mini_batches: 4             # number of mini batches during each learning epoch
  discount_factor: 0.99       # discount factor (gamma)
  lambda: 0.95                # TD(lambda) coefficient for computing returns and advantages
  learning_rate: 0.0003       # learning rate
  learning_rate_scheduler: "KLAdaptiveRL"        # learning rate scheduler class
  learning_rate_scheduler_kwargs:                 # learning rate scheduler kwargs
    kl_threshold: 0.01                           # KL divergence threshold
    min_lr: 0.000001                            # minimum learning rate
  state_preprocessor: null                       # state preprocessor class
  state_preprocessor_kwargs: {}                  # state preprocessor kwargs
  value_preprocessor: null                       # value preprocessor class
  value_preprocessor_kwargs: {}                  # value preprocessor kwargs
  random_timesteps: 0         # random exploration steps
  learning_starts: 0          # learning starts after this many steps
  grad_norm_clip: 1.0        # clipping coefficient for the norm of the gradients
  ratio_clip: 0.2            # clipping coefficient for computing the clipped surrogate objective
  value_clip: 0.2            # clipping coefficient for computing the value loss
  clip_predicted_values: true # clip predicted values during value loss computation
  entropy_loss_scale: 0.01   # entropy loss scaling factor
  value_loss_scale: 1.0      # value loss scaling factor
  kl_threshold: 0.0          # KL divergence threshold for early stopping
  rewards_shaper: null       # rewards shaping function
  time_limit_bootstrap: false # bootstrap at timeout termination
  mixed_precision: true      # enable automatic mixed precision
  experiment:
    write_interval: 10           # TensorBoard writing interval (timesteps)
    checkpoint_interval: "auto"  # interval for checkpoints (timesteps)
    store_separately: false      # whether to store checkpoints separately
    wandb: true                  # whether to use Weights & Biases

env_cfg:
  num_envs: 2048                # number of environments to run in parallel
  episode_length_s: 5.0        # episode length in seconds, this shouldn't be too large, otherwise distance rewards could explode
  dt: 0.01                      # simulation timestep
  step_dt: 0.1                  # control timestep
  agent:
    num_agents: 1               # number of agents
    num_observations: 3         # number of observations per agent
    num_actions: 3              # number of actions per agent
    at_target_threshold: 0.8    # distance threshold for reaching target
    clip_agent_actions: 3.0     # action clipping range
    observation_mode: ["rel_pos"]  # observation mode
    collision_threshold: 0.2     # collision detection threshold
  target:
    clip_target_actions: 2.5    # target action clipping range
  arena:
    use_arena: false           # whether to use arena boundaries
    arena_size: 3.0            # size of arena
    init_dist: 1.0            # initial distance between agent and target, [init_dist, 2 * init_dist]
                              # note that this cannot be too large, otherwise distance rewards might explode
  reward:
    reward_scales:             # reward scaling factors
      distance: -1             # distance to target penalty
      capture: 100.0           # reward for capturing target, 100 cause overfit to moving towards certain direction, 10 doesn't work well 
                                # TODO: find a better way to balance the rewards, maybe increase entropy loss scale? 
  visualize_camera: false      # enable camera visualization
  max_visualize_FPS: 60       # maximum FPS for visualization
  debug_viz: true             # enable debug visualization 